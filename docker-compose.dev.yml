# Docker Compose for llama.cpp development
#
# Usage:
#   Build:     docker compose -f docker-compose.dev.yml build
#   Start:     docker compose -f docker-compose.dev.yml run --rm dev
#   With GPU:  docker compose -f docker-compose.dev.yml run --rm dev
#
# Inside container:
#   rebuild              # Rebuild llama.cpp
#   rebuild -t llama-cli # Build specific target
#   convert-model ...    # Run model conversion
#   llama-cli --help     # Run compiled tools

services:
  dev:
    build:
      context: .
      dockerfile: Dockerfile.dev
      args:
        CUDA_VERSION: "12.6.3"
        UBUNTU_VERSION: "22.04"
        # Set specific GPU arch for faster builds (optional)
        # CUDA_DOCKER_ARCH: "89"  # RTX 40xx
    image: llama-cpp-dev:latest
    container_name: llama-cpp-dev
    volumes:
      # Mount local source for development (your fork/modifications)
      - .:/app/src
      # Mount models directory
      - ~/models:/models
      # Mount HuggingFace cache for faster model downloads
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    stdin_open: true
    tty: true
    working_dir: /app

