#
# Development Dockerfile for llama.cpp model development
# Optimized for DGX Spark (Blackwell GB10 - compute capability 12.1)
#
# Usage:
#   Build:    docker compose -f docker-compose.dev.yml build
#   Run:      docker compose -f docker-compose.dev.yml run --rm dev
#   Rebuild:  (inside container) rebuild
#
ARG UBUNTU_VERSION=22.04
ARG CUDA_VERSION=13.0.1
ARG BASE_CUDA_DEV_CONTAINER=nvcr.io/nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION}

FROM ${BASE_CUDA_DEV_CONTAINER}

# CUDA architecture to build for
# 121 = Blackwell GB10 (DGX Spark)
ARG CUDA_DOCKER_ARCH="121"

# Install build dependencies
RUN apt-get update && \
    apt-get install -y \
        build-essential \
        cmake \
        ninja-build \
        ccache \
        git \
        python3 \
        python3-pip \
        python3-venv \
        libcurl4-openssl-dev \
        libgomp1 \
        curl \
        vim \
        less \
        jq \
        file && \
    rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Clone llama.cpp repository
RUN git clone https://github.com/ggml-org/llama.cpp.git . && \
    git checkout master

# Install Python dependencies for model conversion
RUN pip install --upgrade pip setuptools wheel && \
    pip install --no-cache-dir \
        -r requirements.txt \
        huggingface-hub \
        safetensors \
        torch \
        transformers \
        sentencepiece \
        protobuf \
        tiktoken \
        "gguf>=0.10.0" && \
    pip cache purge

# Configure and build with DGX Spark optimizations
RUN if [ "${CUDA_DOCKER_ARCH}" != "default" ]; then \
        export CMAKE_ARGS="-DCMAKE_CUDA_ARCHITECTURES=${CUDA_DOCKER_ARCH}"; \
    fi && \
    cmake -B build \
        -GNinja \
        -DGGML_CUDA_ENABLE_UNIFIED_MEMORY=1 \
        -DGGML_NATIVE=OFF \
        -DGGML_CUDA=ON \
        -DGGML_BACKEND_DL=ON \
        -DLLAMA_BUILD_TESTS=ON \
        -DCMAKE_BUILD_TYPE=RelWithDebInfo \
        -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \
        ${CMAKE_ARGS} \
        -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined . && \
    cmake --build build --config RelWithDebInfo -j$(nproc)

# Add build/bin to PATH for easy access to compiled tools
ENV PATH="/app/build/bin:${PATH}"
ENV LD_LIBRARY_PATH="/app/build:${LD_LIBRARY_PATH}"

# Create helper scripts
# rebuild: Rebuild llama.cpp from /app/src using same build config
RUN cat > /usr/local/bin/rebuild << 'EOF'
#!/bin/bash
set -e
cd /app/src
if [ "${CUDA_DOCKER_ARCH}" != "default" ]; then
    export CMAKE_ARGS="-DCMAKE_CUDA_ARCHITECTURES=121"
fi
cmake -B build \
    -GNinja \
    -DGGML_CUDA_ENABLE_UNIFIED_MEMORY=1 \
    -DGGML_NATIVE=OFF \
    -DGGML_CUDA=ON \
    -DGGML_BACKEND_DL=ON \
    -DLLAMA_BUILD_TESTS=ON \
    -DCMAKE_BUILD_TYPE=RelWithDebInfo \
    -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \
    ${CMAKE_ARGS} \
    -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined .
cmake --build build --config RelWithDebInfo -j${1:-$(nproc)} "${@:2}"
EOF
RUN chmod +x /usr/local/bin/rebuild

# convert-model: Run model conversion
RUN echo '#!/bin/bash\nset -e\npython3 /app/src/convert_hf_to_gguf.py "$@"' > /usr/local/bin/convert-model && \
    chmod +x /usr/local/bin/convert-model

# Source directory will be mounted here for development
RUN mkdir -p /app/src

WORKDIR /app/src

# Use sh for better docker compatibility (bash available via explicit invocation)
SHELL ["/bin/bash", "-c"]
ENTRYPOINT ["/bin/sh"]
